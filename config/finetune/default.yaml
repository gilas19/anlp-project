# Fine-tuning configuration for FLAN-T5 on CMV debate data
# This configuration file contains all parameters for the fine-tuning process

# Model and data settings
model:
  name: "google/flan-t5-large"
  output_dir: "models/flan-t5-large-cmv-debate-lora"

data:
  path: "data/cmv_10_conversation_trees.json"
  max_samples: null  # Use all available samples, or specify a number
  max_input_length: 512
  max_target_length: 256

# Training hyperparameters
training:
  batch_size: 2
  learning_rate: 1e-4
  num_epochs: 1
  gradient_accumulation_steps: 2
  warmup_steps: 100
  weight_decay: 0.01
  
# LoRA configuration
lora:
  r: 16  # Rank
  lora_alpha: 32  # LoRA scaling parameter
  lora_dropout: 0.1
  target_modules: ["q", "v", "k", "o", "wi", "wo"]  # T5 attention and FFN modules
  bias: "none"

# Evaluation settings
evaluation:
  skip_evaluation: false
  skip_debate_test: false
  eval_steps: 100
  save_steps: 100
  save_total_limit: 3
  
# Logging
logging:
  logging_steps: 50
  report_to: []  # Disable wandb by default