# Fine-tuning configuration for FLAN-T5 on CMV debate data
# This configuration file contains all parameters for the fine-tuning process

# Model and data settings
model:
  name: "google/flan-t5-small"
  output_dir: "models/flan-t5-small-cmv-debate-lora"

data:
  path: "data/cmv_10_conversation_trees.json"
  max_samples: 2
  max_input_length: 512
  max_target_length: 256

# Training hyperparameters
training:
  batch_size: 2
  learning_rate: 0.0001
  num_epochs: 1
  logging_steps: 10
  warmup_steps: 10
  weight_decay: 0.01
  
# LoRA configuration
lora:
  r: 16  # Rank
  lora_alpha: 16  # LoRA scaling parameter
  lora_dropout: 0.1
  target_modules: ["q", "v"]
  bias: "none"

# Evaluation settings
evaluation:
  eval_steps: 50
  save_steps: 50
  save_total_limit: 3
