% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
% \usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
% \usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric, arrows, positioning, fit}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Project title}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
    Shir Babian\qquad Yael Batat\qquad Gilad Ticher \\
    Hebrew University of Jerusalem \\\\
  \texttt{\{shir.babian,yael.batat,gilad.ticher\}@mail.huji.ac.il} \\}

\begin{document}
\maketitle
\begin{abstract}
Current automated fact verification systems rely on single-step classification, potentially missing nuanced reasoning from argumentative discourse. We investigate whether simulated debates between opposing agents improve fact verification accuracy. We fine-tune Flan-T5-large \cite{t5} on Reddit's r/ChangeMyView dataset \cite{winarg} for argumentative reasoning, then simulate debates between agents supporting and refuting claims using FEVER benchmark evidence \cite{FEVER}. Debate transcripts provide context for final verification decisions. Evaluation on FEVER's three-class labels (SUPPORTS, REFUTES, NOT ENOUGH INFO) compares debate-enhanced predictions against baseline methods. Results show modest improvements for the fine-tuned model when using short debate sequences, suggesting that structured argumentative reasoning can enhance automated fact verification under specific conditions.
\end{abstract}

\section{Introduction}

The proliferation of misinformation across digital platforms has emerged as a critical challenge for information integrity and public discourse \cite{vosoughi2018spread}. The exponential growth of user-generated content on social media and news aggregation platforms has outpaced manual fact-checking capabilities, necessitating automated systems for claim verification at scale \cite{kotonya2020explainable}.

Automated fact verification systems have predominantly relied on single-step neural classification approaches, where models directly predict claim veracity based on retrieved evidence \cite{thorne2018fever, augenstein2019multifc}. While these methods have demonstrated competitive performance on established benchmarks, they employ direct inference mechanisms that may not capture the complex reasoning processes inherent in human fact-checking \cite{popat2017truth}. Human fact verification involves deliberative evaluation of multiple perspectives, consideration of counter-arguments, and iterative refinement of conclusions through discourse \cite{toulmin2003uses}.

Recent advances in multi-agent systems and debate-based reasoning have shown promise in enhancing model performance on complex reasoning tasks \cite{du2023improving, liang2023encouraging}. These approaches leverage adversarial interactions between agents to expose weaknesses in reasoning and generate more robust conclusions. However, the application of debate-based methods to fact verification remains underexplored, despite the inherently argumentative nature of claim evaluation.

This work investigates whether simulated debates between opposing model-based agents can improve automated fact verification accuracy compared to direct classification approaches. We fine-tune Flan-T5-large \cite{t5} on argumentative discourse from Reddit's r/ChangeMyView dataset \cite{winarg} to enhance models' capacity for structured argumentation. Subsequently, we deploy these fine-tuned models as opposing agents that engage in debates over claims using evidence from the FEVER benchmark \cite{FEVER}. The debate transcripts serve as enriched context for final verification decisions. Our experimental evaluation on the FEVER test set quantifies the impact of debate-enhanced reasoning on fact verification performance, contributing to understanding how multi-agent argumentative frameworks can advance automated claim assessment.

\section{Data}

Our approach utilizes two distinct datasets: Reddit's r/ChangeMyView for fine-tuning argumentative reasoning capabilities, and the FEVER benchmark for fact verification evaluation and evidence retrieval.

\subsection{r/ChangeMyView Dataset}

We utilize a subset of the r/ChangeMyView dataset \cite{winarg} consisting of 10 structured argumentative discussions from Reddit's r/ChangeMyView subreddit. Each conversation is represented as a tree structure with conversation IDs, titles beginning with "CMV:", original post text, and hierarchical comment threads. Stance annotations (support/refute/no\_stance) for each comment are automatically generated using the all-MiniLM-L6-v2 sentence transformer model \cite{reimers2019sentence} to classify the argumentative position relative to the original claim.

The dataset includes metadata for successful persuasion attempts, identified through DeltaBot confirmations when original posters award "delta" points to comments that effectively changed their perspective. Our fine-tuning process extracts debate training pairs by identifying delta-awarded users and using their arguments as high-quality evidence for persuasive reasoning.

From each conversation tree, we extract prompt-response pairs where the prompt includes the claim, evidence from delta-awarded responses, debate role (support/refute), and conversation history context. Responses are limited to 3 sentences and truncated to 400 characters. We filter out short responses (< 50 characters), system messages, and deleted content, applying text cleaning to remove Reddit formatting, quotes, and links while normalizing whitespace.

\subsection{FEVER Dataset}

We utilize the FEVER Gold Evidence dataset \cite{FEVER} accessed through the copenlu/fever\_gold\_evidence \cite{atanasova2020generating} HuggingFace repository, which provides claims with pre-extracted evidence sentences from Wikipedia. The dataset contains claims annotated as SUPPORTS, REFUTES, or NOT ENOUGH INFO based on evidence from Wikipedia pages.

For our evaluation, we process claims from all available splits (train, validation, test) and extract evidence text from the structured format where each evidence entry contains [title, line\_number, text] tuples. We retain only the text component and concatenate multiple evidence sentences with spaces, truncating to 1000 characters to manage computational constraints.

Evidence snippets are further limited to 500 characters during model inference to ensure consistent prompt lengths across experiments. We evaluate on a subset of 100 claims for computational efficiency during development and testing. The three-class label distribution is preserved, and predictions are standardized through post-processing to ensure they match one of the valid categories (SUPPORTS, REFUTES, NOT ENOUGH INFO).


\section{Methods}

\subsection{Model Architecture and Fine-tuning}
We use the \texttt{google/flan-t5-large} model \cite{chung2022scaling} as the base architecture for all experiments. FLAN-T5 is a sequence-to-sequence language model pretrained with instruction-tuning, making it suitable for multi-step reasoning tasks such as simulated debates. To enhance its ability to produce high-quality argumentative responses, we fine-tuned the model on Reddit's \texttt{r/ChangeMyView} (CMV) discussion data using Low-Rank Adaptation (LoRA) \cite{hu2021lora}.

We applied LoRA adapters to the self-attention layers of the FLAN-T5 model, significantly reducing the number of trainable parameters. The LoRA configuration used rank $r=16$, $\alpha=32$, and dropout rate of 0.1. Fine-tuning was performed using the HuggingFace \texttt{Trainer} API \cite{wolf2020transformers} for 3 epochs with a batch size of 2, a learning rate of $4e{-5}$, and early stopping based on validation loss. The maximum input and target sequence lengths were set to 512 and 256 tokens, respectively. All training was conducted using Google Colab T4 GPU with \texttt{bf16} precision.

\subsection{Prompting and Debate Simulation}
To evaluate the system, we compare two settings: (1) a direct fact verification prompt using claim and evidence only (no-debate), and (2) a simulated debate between two agents who argue opposing positions before a final prediction is made (debate-based). In the debate-based setting, the agents alternate turns (1,2,4,6 each) and are prompted explicitly to support or refute the claim using the evidence, depending on their assigned role. Prompts include instructions enforcing role consistency, dialog history, and reminders to maintain argumentative stance.

After simulating the debate, a final classification prompt summarizes the dialogue and asks the model to output one of three FEVER-compatible labels: \texttt{SUPPORTS}, \texttt{REFUTES}, or \texttt{NOT ENOUGH INFO} \cite{thorne2018fever}.

\subsection{Evaluation Setup}
We evaluate both the baseline (zero-shot) FLAN-T5-Large and our fine-tuned debate model on the FEVER dataset.

Evaluation was conducted over a set of configurations, varying the number of debate turns and the initial speaker (support or refute). For each configuration, we measure accuracy, average generation time, and token length. Final results are saved and analyzed using scripts that compute detailed error distributions, confusion matrices, and correlation statistics across model types and debate strategies.

\section{Results}

We evaluated both baseline and fine-tuned Flan-T5-large models on 100 FEVER claims across multiple configurations, varying debate turns (0, 1, 2, 4, 6) and initiating agents. Initial fine-tuning experiments with 5 epochs showed significant overfitting, with models producing Reddit-style anecdotes rather than structured arguments. Reducing to 2 epochs yielded more task-appropriate responses.

The debate-enhanced approach consistently outperformed direct classification baselines. The fine-tuned model achieved the highest accuracy improvements, particularly with short debates (1-2 turns per agent), showing modest gains over the baseline. Performance plateaued or slightly declined with longer debates (4-6 turns), suggesting diminishing returns from extended argumentation.

Agent role consistency presented challenges, especially for the baseline model. The "refute" agent often produced evasive counterarguments rather than direct contradictions, reflecting the model's bias toward factual accuracy even when instructed to argue against true claims. This "honest by default" behavior was more pronounced in baseline models compared to fine-tuned variants.

Detailed examples, statistical analyses, and error distributions are provided in Appendix~\ref{app:detailed_results}.

\section{Analysis}

\section{Conclusion}

\bibliography{better}
\bibliographystyle{acl_natbib}

\appendix

\section{Detailed Results}
\label{app:detailed_results}

\subsection{Fine-tuning Overfitting Examples}

Our initial experiments with 5 training epochs showed significant overfitting behavior. The model often reproduced Reddit-style personal anecdotes rather than structured arguments. For instance, when prompted with the claim \textit{"Gabrielle Union was in a movie"}, the model responded:

\begin{quote}
\small
\textit{"I saw the movie. The Birth of a Nation was playing at the end of the year. Not sure what's the date, but I'd think it would be around the end of 2016."}
\end{quote}

Similarly, for the claim \textit{"Cosmos: A Spacetime Odyssey secured studio support thanks to Seth MacFarlane"}, the model generated:

\begin{quote}
\small
\textit{"You're comparing a show to a movie, which is the same thing. A movie has a plot and a theme... this is the series to see. There's a good chance it'll change your opinion."}
\end{quote}

These outputs demonstrate loss of task alignment and excessive imitation of training data style. Reducing to 2 epochs produced more evidence-aware, task-appropriate responses.

\subsection{Agent Role Behavior Examples}

Models exhibited reluctance to generate factually incorrect information even when instructed to refute true claims. For example, in a debate about \textit{"Usain Bolt won at the Olympics"}, while the support agent referenced his gold medal achievements, the refute agent responded evasively:

\begin{quote}
\small
\textit{"He didn't win a gold medal in one of the events."}
\end{quote}

This technical accuracy rather than direct contradiction reflects the model's "honest by default" behavior, which was more pronounced in baseline models than fine-tuned variants.

\subsection{Prompt Engineering Evolution}

Initial prompts provided minimal context and resulted in agents repeating claims rather than engaging in genuine argumentation. Our final prompt template included:

\begin{itemize}
    \item Explicit role instructions with consistent stance requirements
    \item Turn-by-turn dialogue history
    \item Behavioral constraints discouraging hedging
    \item Instructions to reference opponent's arguments
    \item Role reminders before each turn
\end{itemize}

These improvements led to more coherent dialogues, though complete role consistency remained challenging.

\end{document}
